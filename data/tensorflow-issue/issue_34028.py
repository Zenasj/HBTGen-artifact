# tf.random.uniform((None, None), dtype=tf.int32) ‚Üê Infer input shape for embedded sequences (batch, timesteps)

import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self, embeddings):
        super(MyModel, self).__init__()
        # Embedding layer initialized with pre-trained embeddings, non-trainable
        self.embedding_layer = tf.keras.layers.Embedding(
            input_dim=embeddings.shape[0],
            output_dim=embeddings.shape[1],
            embeddings_initializer=tf.constant_initializer(embeddings),
            trainable=False
        )
        # GRU layer with dropout and recurrent_dropout
        # Note: dropout & recurrent_dropout cause saving issues in some TF versions
        self.gru_layer = tf.keras.layers.GRU(
            units=embeddings.shape[1],  # Just an assumption; typically should be specified
            dropout=0.5,
            recurrent_dropout=0.5
        )
        self.batchnorm1 = tf.keras.layers.BatchNormalization()
        self.dropout = tf.keras.layers.Dropout(0.5)
        self.dense1 = tf.keras.layers.Dense(256, activation='relu')
        self.batchnorm2 = tf.keras.layers.BatchNormalization()
        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')

    def call(self, inputs, training=False):
        # inputs is expected to be a tuple/list of (x1, x2, x1_mask, x2_mask)
        x1, x2, x1_mask, x2_mask = inputs
        # Embed input sequences
        x1_embed = self.embedding_layer(x1)
        x2_embed = self.embedding_layer(x2)
        # Pass through GRU with masking and training flag
        x1_enc = self.gru_layer(x1_embed, mask=x1_mask, training=training)
        x2_enc = self.gru_layer(x2_embed, mask=x2_mask, training=training)
        # Concatenate encodings
        concat = tf.concat([x1_enc, x2_enc], axis=-1)
        # BatchNorm, Dropout, Dense layers
        out = self.batchnorm1(concat, training=training)
        out = self.dropout(out, training=training)
        out = self.dense1(out)
        out = self.batchnorm2(out, training=training)
        y_pred = self.dense2(out)
        return y_pred

def my_model_function():
    # For demonstration, create dummy embeddings 
    # In real usage, these should come from real pretrained embedding matrix (e.g. word2vec or learned)
    vocab_size = 10000  # assumed vocab size
    embedding_dim = 200  # assumed embedding dimension (matches GRU units)
    embeddings = tf.random.uniform((vocab_size, embedding_dim), dtype=tf.float32)
    return MyModel(embeddings)

def GetInput():
    # Generate dummy data for model input
    # Following inputs: x1, x2 (int sequences), and masks x1_mask, x2_mask (boolean masks)
    batch_size = 4
    max_len1 = 15  # max sequence length for x1
    max_len2 = 17  # max sequence length for x2

    # Integer input sequences with values in [0, vocab_size)
    vocab_size = 10000

    x1 = tf.random.uniform((batch_size, max_len1), minval=0, maxval=vocab_size, dtype=tf.int32)
    x2 = tf.random.uniform((batch_size, max_len2), minval=0, maxval=vocab_size, dtype=tf.int32)

    # Masks: boolean tensors indicating valid timesteps
    # Random for demo, in real life mask is usually generated by tf.sequence_mask according to actual lengths
    x1_mask = tf.sequence_mask(lengths=tf.random.uniform([batch_size], max_len1//2, max_len1, dtype=tf.int32),
                              maxlen=max_len1)
    x2_mask = tf.sequence_mask(lengths=tf.random.uniform([batch_size], max_len2//2, max_len2, dtype=tf.int32),
                              maxlen=max_len2)

    return (x1, x2, x1_mask, x2_mask)

