import random

# -*- coding: utf-8 -*-
"""TensorflowTTS-TFLite.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HudLLpT9CQdh2k04c06bHUwLubhGTWxA

Copyright 2020 The TensorFlow Authors. All Rights Reserved.

Author : [jaeyoo@](https://github.com/jaeyoo),
[khanhlvg@](https://github.com/khanhlvg),
[abattery@](https://github.com/abattery), [thaink@](https://github.com/thaink)
(Google Research) and [dathudeptrai@](https://github.com/dathudeptrai) (owner
TensorflowTTS)

# TensorflowTTS real TFLite Conversion demonstration

This notebook provides a demo to convert models from TensorflowTTS to TFlite

- Github: https://github.com/TensorSpeech/TensorflowTTS
- Audio samples: https://tensorspeech.github.io/TensorflowTTS/
"""

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import os
import sys
import time

import yaml
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow_tts.configs import FastSpeech2Config, FastSpeechConfig, Tacotron2Config
from tensorflow_tts.models import TFFastSpeech, TFFastSpeech2, TFTacotron2
from tensorflow_tts.processor import LJSpeechProcessor
from tensorflow_tts.processor.ljspeech import LJSPEECH_SYMBOLS
from tensorflow_tts.configs import MultiBandMelGANGeneratorConfig

from tensorflow_tts.models import TFMelGANGenerator
from tensorflow_tts.models import TFPQMF

os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
sys.path.append("tensorflowtts/")
tf.__version__


def load_tacotron2():
    # Tacotron2
    with open("tacotron2_config.yml") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = Tacotron2Config(**config["tacotron2_params"])
    tacotron2 = TFTacotron2(
        config=config,
        training=False,
        name="tacotron2v2",
        enable_tflite_convertible=True,
    )

    # Newly added :
    tacotron2.setup_window(win_front=6, win_back=6)
    tacotron2.setup_maximum_iterations(1000)  # be careful

    tacotron2._build()
    tacotron2.load_weights("tacotron2-120k.h5")
    return tacotron2


def load_fastspeech():
    # FastSpeech

    with open("fastspeech_config.yml") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = FastSpeechConfig(**config["fastspeech_params"])
    fastspeech = TFFastSpeech(
        config=config, enable_tflite_convertible=True, name="fastspeech"
    )
    fastspeech._build()
    fastspeech.load_weights("fastspeech-150k.h5")
    return fastspeech


def load_fastspeech2():
    # FastSpeech2
    with open("fastspeech2_config.yml") as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = FastSpeech2Config(**config["fastspeech_params"])
    fastspeech2 = TFFastSpeech2(
        config=config, enable_tflite_convertible=True, name="fastspeech2"
    )
    fastspeech2._build()
    fastspeech2.load_weights("fastspeech2-150k.h5")
    return fastspeech2


def convert_tacotron2(tacotron2):
    """## Convert to TF Lite

    ### Tacotron-2
    """

    # Concrete Function
    tacotron2_concrete_function = tacotron2.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [tacotron2_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open("tacotron2.tflite", "wb") as f:
        f.write(tflite_model)

    print("Model size is %f MBs." % (len(tflite_model) / 1024 / 1024.0))


def convert_fastspeech(fastspeech):
    """### FastSpeech"""

    # Concrete Function
    fastspeech_concrete_function = fastspeech.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [fastspeech_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open("fastspeech_quant.tflite", "wb") as f:
        f.write(tflite_model)

    print("Model size is %f MBs." % (len(tflite_model) / 1024 / 1024.0))


def convert_fastspeech2(fastspeech2):
    """### FastSpeech2"""

    # Concrete Function
    fastspeech2_concrete_function = fastspeech2.inference_tflite.get_concrete_function()

    converter = tf.lite.TFLiteConverter.from_concrete_functions(
        [fastspeech2_concrete_function]
    )
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    tflite_model = converter.convert()

    # Save the TF Lite model.
    with open("fastspeech2_quant.tflite", "wb") as f:
        f.write(tflite_model)

    print("Model size is %f MBs." % (len(tflite_model) / 1024 / 1024.0))


"""# Inference"""


def visualize_mel_spectrogram(mels):
    mels = tf.reshape(mels, [-1, 80]).numpy()
    fig = plt.figure(figsize=(10, 8))
    ax1 = fig.add_subplot(311)
    ax1.set_title("Predicted Mel-after-Spectrogram")
    im = ax1.imshow(np.rot90(mels), aspect="auto", interpolation="none")
    fig.colorbar(mappable=im, shrink=0.65, orientation="horizontal", ax=ax1)
    plt.show()
    plt.close()


def infer_tacotron2():
    """## Tacotron-2"""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="tacotron2.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        return (
            tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),
            tf.convert_to_tensor([len(input_ids)], tf.int32),
            tf.convert_to_tensor([0], dtype=tf.int32),
        )

    # Test the model on random input data.
    def infer(input_text):
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        print("Input ids shape:", input_ids.shape)
        interpreter.resize_tensor_input(input_details[0]["index"], [1, len(input_ids)])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            print("Input tensor %d detail:" % i, detail)
            input_shape = detail["shape"]
            print("Shape of tensor %d:" % i, input_data[i].shape)
            interpreter.set_tensor(detail["index"], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0]["index"]),
            interpreter.get_tensor(output_details[1]["index"]),
        )

    input_text = "Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning."

    decoder_output_tflite, mel_output_tflite = infer(input_text)

    return decoder_output_tflite, mel_output_tflite


def infer_fastspeech():
    """## FastSpeech"""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="fastspeech_quant.tflite")

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)
        return (
            input_ids,
            tf.convert_to_tensor([0], tf.int32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
        )

    # Test the model on random input data.
    def infer(input_text):
        for x in input_details:
            print(x)
        for x in output_details:
            print(x)
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        interpreter.resize_tensor_input(input_details[0]["index"], [1, len(input_ids)])
        interpreter.resize_tensor_input(input_details[1]["index"], [1])
        interpreter.resize_tensor_input(input_details[2]["index"], [1])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            input_shape = detail["shape"]
            interpreter.set_tensor(detail["index"], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0]["index"]),
            interpreter.get_tensor(output_details[1]["index"]),
        )

    input_text = "Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning."

    decoder_output_tflite, mel_output_tflite = infer(input_text)


def infer_fastspeech2():
    """## FastSpeech2"""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="fastspeech2_quant.tflite")

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        input_ids = tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0)
        return (
            input_ids,
            tf.convert_to_tensor([0], tf.int32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
            tf.convert_to_tensor([1.0], dtype=tf.float32),
        )

    # Test the model on random input data.
    def infer(input_text):
        for x in input_details:
            print(x)
        for x in output_details:
            print(x)
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        interpreter.resize_tensor_input(input_details[0]["index"], [1, len(input_ids)])
        interpreter.resize_tensor_input(input_details[1]["index"], [1])
        interpreter.resize_tensor_input(input_details[2]["index"], [1])
        interpreter.resize_tensor_input(input_details[3]["index"], [1])
        interpreter.resize_tensor_input(input_details[4]["index"], [1])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            input_shape = detail["shape"]
            interpreter.set_tensor(detail["index"], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0]["index"]),
            interpreter.get_tensor(output_details[1]["index"]),
        )

    input_text = "Recent research at Harvard has shown meditating\
    for as little as 8 weeks, can actually increase the grey matter in the \
    parts of the brain responsible for emotional regulation, and learning."

    decoder_output_tflite, mel_output_tflite = infer(input_text)


def convert_vocoder():
    # Convert the vocoder as a TFLite model.
    with open(
        "tensorflowtts/examples/multiband_melgan/conf/multiband_melgan.v1.yaml"
    ) as f:
        config = yaml.load(f, Loader=yaml.Loader)

    config = MultiBandMelGANGeneratorConfig(
        **config["multiband_melgan_generator_params"]
    )

    class TFMBMelGANGenerator(TFMelGANGenerator):
        def __init__(self, config, **kwargs):
            super().__init__(config, **kwargs)
            self.pqmf = TFPQMF(config=config, name="pqmf")

        def build(self, input_shape):
            self.pqmf.build(input_shape)
            self.built = True

        @tf.function(
            experimental_relax_shapes=True,
            input_signature=[tf.TensorSpec(shape=[None, None, 80], dtype=tf.float32)],
        )
        def call(self, mels):
            mb_audios = super().call(mels)
            audios = self.pqmf.synthesis(mb_audios)
            return audios

    mb_melgan = TFMBMelGANGenerator(config, name="mb_melgan")
    fake_mels = tf.random.uniform(shape=[1, 256, 80], dtype=tf.float32)
    audios = mb_melgan(fake_mels)[0, :, 0]
    mb_melgan.summary()
    mb_melgan.load_weights("generator-940000.h5")
    concrete_func = mb_melgan.call.get_concrete_function()
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_ops = [
        tf.lite.OpsSet.TFLITE_BUILTINS,
        tf.lite.OpsSet.SELECT_TF_OPS,
    ]
    mbmlite = converter.convert()
    # Save the TF Lite model.
    with tf.io.gfile.GFile("mbm_quant.tflite", "wb") as f:
        f.write(mbmlite)
    print("Model saved to mbm_quant.tflite file")
    print("Model size is %f MBs." % (len(mbmlite) / 1024 / 1024.0))


def infer_mbmelgan(mel_output_tflite):
    class MBMelganLite:
        def __init__(self, path):
            # Load TFLite model and allocate tensors.
            self.interpreter = tf.lite.Interpreter(model_path=path)
            # Get input and output tensors.
            self.inputs = self.interpreter.get_input_details()
            self.outputs = self.interpreter.get_output_details()
            self.current_shape = self.inputs[0]["shape"]

        def printDetails(self):
            for x in self.inputs:
                print(x)
            for x in self.outputs:
                print(x)

        def infer(self, mel):
            # Resize if input length different, assuming batch size is always 1.
            if self.current_shape[1] is not mel.shape[1]:
                print(
                    f"Input shape: {mel.shape} , interpreter shape: {self.current_shape}"
                )
                print("Warning: Latency might be affected due to change in input shape")
                self.interpreter.resize_tensor_input(
                    self.inputs[0]["index"], list(mel.shape)
                )
                self.interpreter.allocate_tensors()
                self.current_shape = mel.shape
            self.interpreter.set_tensor(self.inputs[0]["index"], mel)
            self.interpreter.invoke()
            tflite_results = self.interpreter.get_tensor(self.outputs[0]["index"])
            return tflite_results

    mbmelganlite = MBMelganLite(path="mbm_quant.tflite")

    start = time.time()
    audio_mblite = mbmelganlite.infer(mel_output_tflite)[0, :, 0]
    print(f"Time taken: {time.time() - start}s")


if __name__ == "__main__":
    # Load the synthesizer and vocoder models, convert them to TFLite, and save to disk.
    tacotron2 = load_tacotron2()
    convert_tacotron2(tacotron2)
    convert_vocoder()

    # TFLite inference.
    decoder_output_tflite, mel_output_tflite = infer_tacotron2()
    print("Type of ``mel_output_tflite``:", type(mel_output_tflite))
    print("Value of ``mel_output_tflite``:", mel_output_tflite)
    infer_mbmelgan(mel_output_tflite)

def infer_tacotron2():
    """## Tacotron-2"""
    # Load the TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path="tacotron2.tflite")
    interpreter.allocate_tensors()

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Prepare input data.
    def prepare_input(input_ids):
        return (
            tf.expand_dims(tf.convert_to_tensor(input_ids, dtype=tf.int32), 0),
            tf.convert_to_tensor([len(input_ids)], tf.int32),
            tf.convert_to_tensor([0], dtype=tf.int32),
        )

    # Test the model on random input data.
    def infer(input_text):
        processor = LJSpeechProcessor(None, symbols=LJSPEECH_SYMBOLS)
        input_ids = processor.text_to_sequence(input_text.lower())
        print("Input ids shape:", input_ids.shape)
        interpreter.resize_tensor_input(input_details[0]["index"], [1, len(input_ids)])
        interpreter.allocate_tensors()
        input_data = prepare_input(input_ids)
        for i, detail in enumerate(input_details):
            print("Input tensor %d detail:" % i, detail)
            input_shape = detail["shape"]
            print("Shape of tensor %d:" % i, input_data[i].shape)
            interpreter.set_tensor(detail["index"], input_data[i])

        interpreter.invoke()

        # The function `get_tensor()` returns a copy of the tensor data.
        # Use `tensor()` in order to get a pointer to the tensor.
        return (
            interpreter.get_tensor(output_details[0]["index"]),
            interpreter.get_tensor(output_details[1]["index"]),
        )