# tf.random.uniform((None, None)) ‚Üê Input shape is ambiguous from the issue; using a placeholder shape (batch_size, features)

import tensorflow as tf

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        # We implement two submodules:
        # 1) A tf.Module based layer similar to MockLayer using tf.Variable directly
        # 2) A Keras Layer-based submodule using LayerNormalization as in the example
        
        # Submodule 1: tf.Module style with tf.Variable tracking working correctly
        class ModuleLayer(tf.Module):
            def __init__(self):
                super().__init__()
                # Variables initialized randomly
                self.m = tf.Variable(tf.random.normal([5, 5]), name="m")
                self.w = tf.Variable(tf.random.normal([5, 5]), name="w")

            def __call__(self, inputs):
                # Simple multiply m with inputs (broadcasting assumed or inputs compatible)
                return self.m * inputs
        
        # Submodule 2: Keras Layer that no longer inherits tf.Module (Keras 3 style),
        # so its variables aren't tracked by tf.Module reflection
        class KerasNormLayer(tf.keras.layers.Layer):
            def __init__(self):
                super().__init__()
                self.norm = tf.keras.layers.LayerNormalization()

            def call(self, inputs):
                return self.norm(inputs)

        # Instantiate submodules
        self.module_layer = ModuleLayer()
        self.keras_norm = KerasNormLayer()

    def call(self, inputs):
        # Forward pass applies both submodules to the inputs
        # Then returns a dictionary comparing their outputs to illustrate difference
        
        output_module = self.module_layer(inputs)
        output_keras_norm = self.keras_norm(inputs)
        
        # Compute elementwise difference for demonstration
        difference = tf.abs(output_module - output_keras_norm)
        
        # Return the tuple of results plus the difference
        return {
            "module_output": output_module,
            "keras_norm_output": output_keras_norm,
            "difference": difference
        }


def my_model_function():
    # Return an instance of MyModel
    # No pretrained weights are involved from the issue context
    return MyModel()


def GetInput():
    # Based on the example and layers, inputs must be compatible with:
    # - ModuleLayer: expects a tensor broadcastable with shape [5,5]
    # - Keras LayerNormalization: inputs usually float tensor with shape [batch, features]
    #
    # For simplicity, let's assume input shape of (batch_size=2, 5, 5)
    # which aligns with the variable shape in MockLayer and is acceptable to LayerNormalization
    # LayerNormalization normalizes across last axis by default

    input_tensor = tf.random.uniform(shape=(2, 5, 5), dtype=tf.float32)
    return input_tensor

