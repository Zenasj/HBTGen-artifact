# tf.random.uniform((1, 224, 224, 3), dtype=tf.float32) ‚Üê inferred input shape from MobileNetV2 example and custom model in issue

import tensorflow as tf
from tensorflow import keras
import tensorflow_model_optimization as tfmot

class MyModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        # Base conv layer (non-quantized)
        self.conv_fp = keras.layers.Conv2D(32, (3, 3), activation='relu')

        # Quantized Conv2D layer annotated with quantize_annotate_layer
        conv_to_quant = keras.layers.Conv2D(32, (3, 3), activation='relu')
        self.conv_q_annotated = tfmot.quantization.keras.quantize_annotate_layer(conv_to_quant)

        # Quantize Apply wrapper for the annotated conv layer only
        # Since quantize_apply expects a Model or Sequential, we wrap the conv_q_annotated layer here
        self.q_aware_submodel = tf.keras.Sequential([self.conv_q_annotated])

        self.concat = keras.layers.Concatenate()
        self.pool = keras.layers.MaxPool2D(pool_size=(5, 5), strides=5)
        self.flatten = keras.layers.Flatten()
        self.dense = keras.layers.Dense(2)

        # Apply quantize_apply to the entire combined model after building call graph,
        # but for this standalone Model subclass, the pattern below is an approximation
        # QuantizeWrapper is added only to annotated layers via this approach.
        # The user typically will quantize_apply over a Model, but here we do it for conv_q layer only.

        # However, to keep logic clean and consistent,
        # we will rely on functional calls in call() and the quantization happens on conv_q_annotated layer.

    def call(self, inputs, training=False):
        # FP conv layer path
        x1 = self.conv_fp(inputs)

        # Quantized conv layer path applied only on second branch
        # Here we call the quantized conv layer via the quantize_apply wrapper
        # Because in standalone class it's tricky to replicate quantize_apply,
        # we directly call the annotated conv layer here.
        x2 = self.q_aware_submodel(inputs, training=training)

        # Concatenate both conv outputs
        x = self.concat([x1, x2])
        x = self.pool(x)
        x = self.flatten(x)
        x = self.dense(x)
        return x

def my_model_function():
    """
    Returns an instance of MyModel.
    This model mimics the manually constructed example in the issue:
    - One Conv2D layer unquantized.
    - One Conv2D layer quantize-annotated and wrapped inside tfmot.quantize_apply.
    - Outputs concatenated and passed through further layers.
    """
    # Note: Full quantize_apply wrapping happens normally over the entire Model after building functional model.
    # Here we rely on quantize_annotate_layer + quantize_apply on the submodel inside MyModel.
    return MyModel()

def GetInput():
    """
    Returns a random input tensor compatible with MyModel.
    Input shape inferred from examples is (1, 224, 224, 3), dtype float32.

    Shape matches standard MobileNetV2 input used in the issue.
    """
    return tf.random.uniform((1, 224, 224, 3), dtype=tf.float32)

