# tf.random.uniform((B, H, W, C), dtype=tf.float32)  â† Assumed input shape: batch size B unknown, height H, width W, channels C

import tensorflow as tf
import numpy as np

# Placeholder binarize function since the original binary_ops.binarize is not provided
def binarize(x, H=1.0):
    # Simple sign binarization scaled by H (like BinaryNet)
    # Values >0 -> H, <=0 -> -H
    return H * tf.sign(x)

# Constraint to clip weights within a range [min_value, max_value]
class Clip(tf.keras.constraints.Constraint):
    def __init__(self, min_value, max_value):
        self.min_value = min_value
        self.max_value = max_value

    def __call__(self, w):
        return tf.clip_by_value(w, self.min_value, self.max_value)

    def get_config(self):
        return {'min_value': self.min_value, 'max_value': self.max_value}


class MyModel(tf.keras.Model):
    '''
    Fused model inspired by BinaryConv2D custom convolution layer using binarized kernel.
    Implements a binarized Conv2D-like operation by binarizing the kernel weights,
    then multiplying with the input tensor element-wise for demonstration.
    '''

    def __init__(self, filters, kernel_size, H=1., use_bias=True, activation=None, data_format='channels_last', **kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size if isinstance(kernel_size, (tuple,list)) else (kernel_size, kernel_size)
        self.H = H
        self.use_bias = use_bias
        self.activation = tf.keras.activations.get(activation)
        self.data_format = data_format

        # Initialize kernel_lr_multiplier and bias_lr_multiplier as placeholders
        self.kernel_lr_multiplier = H
        self.bias_lr_multiplier = None

        # Layers / weights will be created in build()
        self.built = False

    def build(self, input_shape):
        # Determine channel axis based on data format
        if self.data_format == 'channels_first':
            channel_axis = 1
        else:
            channel_axis = -1

        input_dim = input_shape[channel_axis]
        if input_dim is None:
            raise ValueError('The channel dimension of the inputs should be defined. Found None.')

        kernel_shape = self.kernel_size + (input_dim, self.filters)

        # Compute Glorot-based H and kernel_lr_multiplier if needed (example logic)
        base = self.kernel_size[0] * self.kernel_size[1]
        nb_input = int(input_dim * base)
        nb_output = int(self.filters * base)
        if self.H == 'Glorot':
            self.H = np.float32(np.sqrt(1.5 / (nb_input + nb_output)))

        if self.kernel_lr_multiplier == 'Glorot':
            self.kernel_lr_multiplier = np.float32(1. / np.sqrt(1.5 / (nb_input + nb_output)))

        # Store calculated H
        H = self.H if isinstance(self.H, float) else 1.0

        # Constraints and initializers
        self.kernel_constraint = Clip(-H, H)
        self.kernel_initializer = tf.keras.initializers.RandomUniform(-H, H)

        # Define kernel weights that will be binarized during call
        self.kernel = self.add_weight(
            shape=kernel_shape,
            initializer=self.kernel_initializer,
            name='kernel',
            constraint=self.kernel_constraint,
            trainable=True,
        )

        if self.use_bias:
            self.bias = self.add_weight(
                shape=(self.filters,),
                initializer='zeros',
                name='bias',
                trainable=True,
            )
        else:
            self.bias = None

        self.input_spec = tf.keras.layers.InputSpec(ndim=4, axes={channel_axis: input_dim})
        self.built = True

    @tf.function
    def call(self, inputs):
        '''
        Forward pass:
        - Binarize the kernel weights (sign + scaling)
        - Perform element-wise multiplication with inputs by broadcasting the binarized kernel.
        - Add bias if used.
        - Apply activation if any.
        '''

        # Binarize kernel
        binary_kernel = binarize(self.kernel, H=self.H)

        # binary_kernel shape: (kernel_h, kernel_w, input_channels, filters)
        # Here as an example, we simulate a simplified operation:
        # Instead of doing convolution (which is standard), we do elementwise multiplication of input
        # with a broadcasted kernel slice for demonstration due to original code's experimental approach.

        # To emulate original code snippet that did:
        # bk_temp = np.reshape(K.eval(binary_kernel[:,:,:,0]), (-1,kernel_h,kernel_h,1))
        # bk_cube = zeros(30,30,30,1)
        # bk_cube[:] = bk_temp
        # outputs = inputs * bk_cube
        #
        # We'll simulate with tf ops and avoid eager eval.
        #
        # Assumptions:
        # - input shape: [B, H, W, C]
        # - kernel: [kh, kw, Cin, filters]
        # We'll take the first filter slice (kernel[:,:,:,0]) and broadcast multiply with input
        # Also assume input height, width are >= 30 for this example.
        # Use tf.tile or tf.broadcast_to to create bk_cube.

        kh, kw, cin, filt = binary_kernel.shape

        # Extract kernel slice for first filter: shape (kh, kw, cin)
        bk_slice = binary_kernel[:, :, :, 0]  # Shape: (kh, kw, cin)

        # Reshape bk_slice to (kh, kw, cin, 1) for broadcasting, but input is (B, H, W, C)
        # We want to broadcast bk_slice in input spatial dims:
        # We'll tile bk_slice to approx match spatial dimension 30x30 (as per original)
        # Assume inputs.shape[1] and inputs.shape[2] (height and width) >= 30

        input_shape = tf.shape(inputs)
        B = input_shape[0]
        H = input_shape[1]
        W = input_shape[2]
        C = input_shape[3]

        # Create a "kernel cube" of shape ~ (H, W, C) filled by the kernel slice (broadcast appropriately)
        # First, reshape bk_slice to (kh, kw, cin), then tile to match (H, W, C)
        # For simplification, tile kernel slice to (H, W, C) - this is a rough emulation

        # For safety in dynamic shapes, use tf.tile carefully
        # Also consider kernel slice shape can be smaller than input spatial size

        # We resize kernel slice spatial dims to input H,W by tiling or padding

        # Pad or tile kernel slice to shape (H, W, cin)
        # Calculate tile multiples for height and width
        tile_h = tf.math.floordiv(H + kh - 1, kh)
        tile_w = tf.math.floordiv(W + kw - 1, kw)

        tiled_kernel = tf.tile(bk_slice, [tile_h, tile_w, 1])  # shape (kh*tile_h, kw*tile_w, cin)
        resized_kernel = tiled_kernel[:H, :W, :]  # crop to exact input spatial dims

        # Now resized_kernel has shape (H, W, C)
        # Expand dims to (1, H, W, C) for broadcasting with inputs (B,H,W,C)
        bk_cube = tf.expand_dims(resized_kernel, 0)

        # Elementwise multiply inputs and bk_cube
        outputs = inputs * bk_cube

        if self.use_bias:
            # Use built-in bias_add, assuming channels_last
            outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')

        if self.activation is not None:
            outputs = self.activation(outputs)

        return outputs

    def get_config(self):
        config = super().get_config()
        config.update({
            'filters': self.filters,
            'kernel_size': self.kernel_size,
            'H': self.H,
            'use_bias': self.use_bias,
            'activation': tf.keras.activations.serialize(self.activation),
            'data_format': self.data_format,
        })
        return config

def my_model_function():
    # Create an instance of MyModel with some example parameters
    # Here:
    # filters=1 (to match kernel slice logic),
    # kernel_size=3 (typical conv kernel),
    # H = 1.0 scaling factor,
    # use_bias=True,
    # activation='relu'
    return MyModel(filters=1, kernel_size=3, H=1.0, use_bias=True, activation='relu')

def GetInput():
    # Return random input tensor matching expected shape for MyModel
    # Assumptions:
    # - batch size: 4
    # - height: 30
    # - width: 30
    # - channels: 3

    B = 4
    H = 30
    W = 30
    C = 3
    return tf.random.uniform((B, H, W, C), dtype=tf.float32)

