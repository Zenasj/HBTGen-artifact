# tf.random.uniform((B, H, W, C), dtype=tf.float32) ‚Üê Assuming input shape [batch, height, width, channels]

import tensorflow as tf
import functools

class MyModel(tf.keras.Model):
    """
    A custom Conv2D layer wrapper that addresses the issue of caching input shape and 
    convolution operation for dilated convolutions, inspired by the workaround for Conv1D 
    presented in the issue. This model supports arbitrary input sizes for dilated convolution 
    and recreates the convolution operation dynamically to prevent errors due to fixed input shape assumptions.
    """
    def __init__(self, filters=4, kernel_size=3, padding="SAME", dilation_rate=4, data_format="channels_last", **kwargs):
        super().__init__(**kwargs)
        self.filters = filters
        self.kernel_size = kernel_size
        self.padding = padding
        self.dilation_rate = dilation_rate
        self.data_format = data_format
        self.rank = 2  # 2D conv
        # Initialize weights here rather than relying on Conv2D layer's build
        self.conv_kernel = None
        self.bias = None

    def build(self, input_shape):
        # input_shape: [batch, height, width, channels]
        channel_axis = -1 if self.data_format == "channels_last" else 1
        input_dim = int(input_shape[channel_axis])
        kernel_shape = (self.kernel_size, self.kernel_size, input_dim, self.filters)

        # Create convolution kernel variable
        self.conv_kernel = self.add_weight(
            name="kernel",
            shape=kernel_shape,
            initializer=tf.keras.initializers.GlorotUniform(),
            trainable=True,
            dtype=self.dtype
        )
        self.bias = self.add_weight(
            name="bias",
            shape=(self.filters,),
            initializer='zeros',
            trainable=True,
            dtype=self.dtype
        )
        super().build(input_shape)

    def call(self, inputs, training=None):
        # Dynamically create the convolution operation on each call to avoid fixing input shape
        if self.padding == 'causal':
            op_padding = 'VALID'  # Conv1D causal handled here; not typical for 2D
        else:
            op_padding = self.padding.upper()

        # TensorFlow expects data_format as 'NHWC' or 'NCHW'
        tf_data_format = tf.keras.utils.conv_utils.convert_data_format(self.data_format, self.rank + 2)

        # Create the functional convolution operation with the current attributes
        conv_op = functools.partial(
            tf.nn.convolution,
            strides=[1, 1],
            padding=op_padding,
            data_format=tf_data_format,
            dilations=[self.dilation_rate, self.dilation_rate]
        )

        # Perform convolution using functional conv_op
        outputs = conv_op(inputs, self.conv_kernel)
        outputs = tf.nn.bias_add(outputs, self.bias, data_format=tf_data_format)

        return outputs

def my_model_function():
    # Return an instance of MyModel with default parameters defined in the issue example
    return MyModel(filters=4, kernel_size=3, padding="SAME", dilation_rate=4)

def GetInput():
    # Produces a random input tensor matching the expected input shape by MyModel.
    # As the original issue involves inputs with channels=1 and spatial sizes like 32x32 or 41x41,
    # we produce a sample tensor with shape [1, 41, 41, 1] (batch=1, height=41, width=41, channels=1).
    # dtype float32 to be compatible with Conv2D weights.
    return tf.random.uniform((1, 41, 41, 1), dtype=tf.float32)

