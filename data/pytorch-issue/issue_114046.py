import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import os
import torch.nn.functional as F
gpu_device = torch.device("cuda")
cpu_device = torch.device("cpu")
class PreprocessAndCalculateModel(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        device = x.device
        output = torch.arctanh(x)
        return output

real_inputs = torch.Tensor([[-5.9331e-01, -6.0305e-01, -4.5996e-01, -7.1218e-01, -8.1896e-01,
         -7.3962e-01, -8.1463e-01, -2.2379e-01, -1.0368e+00, -2.3112e-01,
         -1.2557e+00, -2.6255e-01, -7.1644e-01, -5.8074e-01, -3.2793e-01,
         -4.3168e-01, -2.5361e-01, -4.9615e-01, -1.1695e+00, -9.8681e-01,
         -4.3377e-01, -6.3595e-01, -1.1639e+00, -4.5603e-01, -3.1878e-01,
         -7.5838e-01, -8.2477e-01, -3.2992e-01, -1.7724e-01, -8.6040e-01,
         -6.9043e-01, -9.1152e-01, -3.2625e-01, -4.0982e-01, -2.2198e-01,
         -3.8905e-02, -8.7414e-01, -1.0169e+00, -7.3292e-01, -9.1185e-01,
         -8.6688e-01, -6.4961e-01, -9.5093e-01, -4.3481e-01, -8.9533e-01,
         -6.0761e-01, -4.8593e-02, -8.3187e-01, -8.0657e-01, -1.0012e+00,
         -8.2039e-01, -2.3405e-01, -4.6539e-01, -7.6661e-01, -8.4774e-01,
         -8.8612e-01, -1.1951e+00, -4.6041e-01, -4.4869e-01, -9.8927e-01,
         -8.5493e-01, -7.5628e-01, -5.0515e-01, -6.4105e-01, -2.4567e-01,
         -5.8766e-01, -3.5653e-01, -1.1559e-01, -5.1318e-01, -3.3893e-01,
         -5.1584e-01, -7.5485e-01, -1.0686e+00, -4.1829e-01, -6.2099e-01,
         -1.0093e+00, -7.8813e-01, -7.4737e-01, -2.5753e-01, -5.8666e-01,
         -4.6633e-01, -4.6853e-01, -2.6967e-01, -1.0998e+00, -6.6200e-01,
         -3.1433e-01, -1.2844e+00, -4.0707e-01, -7.5134e-01, -5.0906e-01,
         -3.4428e-01, -6.3913e-01, -7.9118e-01, -3.3408e-01, -3.1923e-01,
         -8.0676e-01, -6.7852e-01, -5.9072e-01, -9.5510e-01, -8.0246e-01,
         -7.4719e-01, -1.1986e+00, -1.0238e+00, -9.0603e-01, -9.9834e-01,
         -1.1331e-01, -9.6727e-01, -5.6069e-01, -9.0598e-01, -8.1345e-01,
         -7.7101e-01, -3.7550e-01, -8.7662e-01, -7.3005e-01, -7.0829e-01,
         -7.7400e-01, -9.2710e-01, -8.2785e-01, -7.0606e-01, -1.0812e+00,
         -1.1594e+00, -7.7002e-01, -7.7795e-01, -6.0856e-01, -1.1607e+00,
         -1.0346e+00, -5.0865e-01, -8.4846e-01, -1.0528e+00, -6.5901e-01,
         -6.2690e-01, -5.0201e-01, -7.5337e-01, -3.1864e-01, -8.7563e-01,
         -8.4875e-01, -6.0225e-01, -7.9373e-01, -6.6053e-01, -5.2425e-01,
         -6.5071e-01, -3.9761e-01, -4.6562e-01, -8.0717e-01, -8.7333e-01,
         -1.1993e+00, -8.9223e-01, -7.2547e-01, -3.3000e-01, -5.7541e-01,
         -4.9331e-01, -4.6567e-01, -7.4694e-01, -8.9106e-01, -2.8842e-01,
         -6.9557e-01, -6.0201e-01, -8.9703e-01, -8.6449e-01, -5.1594e-01,
         -1.7415e-01, -2.0633e-01, -1.2241e+00, -8.1305e-01, -1.1619e+00,
         -4.5757e-01, -6.2828e-01, -5.5871e-01, -4.9428e-01, -1.1629e+00,
         -4.0686e-01, -3.2539e-01, -8.3428e-01, -6.0341e-01, -3.5260e-01,
         -7.1580e-01, -1.2744e+00, -3.3784e-01, -2.4902e-01, -8.0497e-01,
         -8.4147e-01, -1.7186e-01, -2.9333e-01, -3.4709e-01, -5.0250e-01,
         -9.4034e-01, -9.3551e-01, -1.3509e+00, -2.3607e-01, -3.3692e-01,
         -1.0097e+00, -8.8541e-01],
        [ 0.0000e+00,  3.7405e-01, -2.6903e-01,  3.5683e-01,  3.6862e-01,
         -3.5669e-01, -2.3553e-01, -1.9795e-01,  1.3969e-01,  5.2755e-02,
          4.3611e-02,  1.1862e-01,  4.6250e-01, -5.7581e-01, -4.7967e-02,
          1.9266e-01,  1.2043e-01, -3.8322e-01, -1.3943e-01, -3.3260e-01,
         -1.5521e-01,  2.6032e-01,  6.0880e-02,  1.3783e-01,  1.6394e-01,
          2.1570e-01, -1.3512e-01,  2.8031e-02, -7.0897e-02,  4.5987e-01,
          1.0727e-02, -5.0617e-01, -6.2020e-02,  1.2375e-01, -2.0366e-01,
          1.0624e-02,  3.3580e-01, -6.9571e-04, -2.5588e-01, -3.6614e-02,
         -4.9908e-01, -3.8363e-01,  1.9807e-01,  1.4466e-01,  4.4007e-01,
         -4.1307e-01, -4.8255e-03,  4.4733e-01,  7.1690e-02, -2.0623e-01,
         -3.6077e-02, -1.7532e-01,  1.6789e-01, -3.4883e-01, -4.1109e-01,
          8.8560e-02,  1.6392e-02,  9.6498e-03,  2.8704e-01, -2.3084e-01,
          3.7688e-01,  4.8739e-02,  2.5219e-02, -3.5186e-01,  1.9907e-01,
          1.1153e-01,  6.0506e-02,  8.8241e-02, -4.3762e-01,  7.0517e-02,
          3.0473e-01,  4.1774e-01, -2.3233e-01,  2.6343e-01, -1.7123e-01,
         -5.6838e-02,  1.1087e-01,  1.9873e-01,  2.7947e-01, -3.3617e-01,
          1.7957e-02,  2.3450e-02, -1.9208e-01, -1.3583e-02, -3.6766e-01,
         -8.3491e-02,  1.9386e-02, -5.9089e-02, -3.6386e-01, -2.2159e-01,
         -1.9620e-01, -1.5220e-01,  1.7431e-01,  2.8682e-01, -2.2750e-01,
         -2.2783e-01,  2.0348e-01,  4.5333e-01, -3.8029e-01, -1.0111e-01,
         -3.6314e-01, -5.1604e-02, -1.9461e-01, -2.8976e-01, -1.0268e-01,
         -5.9536e-02, -2.2209e-01, -2.3433e-03,  3.1359e-01, -2.2868e-01,
         -2.9693e-01, -5.5838e-02,  2.5903e-01, -5.8774e-01, -5.8558e-01,
         -1.1253e-01, -1.8279e-01,  4.9552e-01, -3.2277e-02, -5.7191e-02,
         -1.0257e-02,  5.7938e-01, -2.2028e-01, -1.0780e-02,  3.2408e-02,
          2.4398e-01, -4.2775e-01,  4.0865e-01,  1.0040e-01,  7.0304e-02,
          6.2675e-01, -9.4072e-02, -5.2664e-01,  3.0927e-01,  1.6067e-01,
         -2.9514e-01, -4.9250e-01, -1.5920e-01, -5.2067e-01,  4.6609e-01,
          5.7641e-01,  1.3582e-01, -1.0416e-01,  4.9128e-02,  5.1013e-01,
          3.0300e-02,  2.3001e-01, -4.0463e-01, -2.8042e-01,  3.4765e-01,
          3.0590e-01, -1.3034e-01,  4.6463e-01, -2.3914e-02,  2.8768e-01,
         -4.1160e-01, -1.4640e-01, -2.5984e-01,  4.4418e-01,  2.4285e-01,
          1.7296e-02,  7.2107e-04, -6.6483e-02, -1.5402e-01,  7.7958e-02,
          3.1979e-01, -5.3219e-01, -2.4519e-02,  1.8690e-01, -2.0844e-01,
          2.0062e-01, -2.0377e-01, -3.6471e-01, -2.7728e-02,  5.5711e-02,
          5.9900e-01, -6.6685e-02, -2.9909e-01,  7.1200e-02, -3.0876e-01,
          3.4065e-01,  9.6738e-02, -1.2055e-01, -1.8226e-01, -4.5109e-01,
          2.1220e-01,  7.6303e-02,  4.8108e-02, -1.2470e-01, -2.7641e-01,
         -1.2444e-01,  2.9382e-01]])
model = PreprocessAndCalculateModel()
x = real_inputs
output_gpu = model.to(gpu_device)(x.cuda())
output_cpu = model.to(cpu_device)(x.cpu())
print(torch.allclose(output_gpu.cpu(), output_cpu, atol=1e-1))