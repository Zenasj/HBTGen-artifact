import torch.nn as nn

# test_scaled_dot_product_attention.py   

import torch
import torch.nn.functional as F
from torch.nn.attention import sdpa_kernel, SDPBackend
from torch import inf

device='cuda'
q = torch.tensor([[[[-0.3313,  0.5768],
          [ 0.1334,  0.6123],
          [-0.6594, -0.1936],
          [-0.4009, -1.1215]],

         [[-0.2542,  0.2620],
          [-0.1803, -0.7368],
          [ 0.4471,  0.2735],
          [ 0.1007, -0.2059]],

         [[-0.3820,  0.4473],
          [-0.3855,  0.5480],
          [-0.5695,  0.0879],
          [-0.8565, -0.8578]],

         [[ 0.1422,  0.4125],
          [ 0.2482, -0.2683],
          [ 0.1945, -0.8867],
          [ 1.1332, -0.1735]]],


        [[[ 0.5657,  0.2606],
          [ 0.8316, -0.1097],
          [ 0.1557,  1.1108],
          [-0.3531,  0.4189]],

         [[ 0.3400,  0.3829],
          [-0.2595,  0.2666],
          [-1.3184, -0.0565],
          [ 0.3332,  0.1775]],

         [[ 0.4977,  0.2799],
          [ 0.3651, -0.1559],
          [ 1.0253, -1.3255],
          [ 0.0642, -1.4244]],

         [[-0.6632, -0.2317],
          [-0.0400,  0.2458],
          [-0.4907, -0.7546],
          [-0.2615, -2.0718]]]], device=device)
k = torch.tensor([[[[ 1.0370,  0.4798],
          [ 0.7290, -0.1673],
          [-0.4613, -0.5825],
          [ 0.1058,  0.1304]],

         [[ 0.5682,  0.2530],
          [ 0.0241,  0.2535],
          [ 0.1805, -0.2237],
          [-0.2210,  0.6242]],

         [[ 0.0327, -0.3588],
          [ 0.4650, -0.4627],
          [ 0.1244,  0.4913],
          [ 0.3758,  0.4677]],

         [[-0.2546,  0.9355],
          [ 0.2496,  0.6018],
          [ 1.0947, -1.1829],
          [ 1.0116, -0.8344]]],


        [[[-0.4283, -0.5132],
          [-0.0276, -0.2733],
          [ 0.2496,  0.1805],
          [-0.0715, -0.2410]],

         [[ 0.0696, -0.5883],
          [-0.0755, -0.3164],
          [-0.7018, -0.2908],
          [-0.3276, -0.2789]],

         [[-0.0737,  0.4240],
          [ 0.4368,  1.0251],
          [ 0.4959,  0.2148],
          [ 0.1271,  0.0966]],

         [[-0.4718, -0.0114],
          [-0.1748,  0.0177],
          [ 0.8074, -0.8456],
          [ 1.2021, -1.6366]]]], device=device)

v = torch.tensor([[[[ 1.1711e-01,  9.7816e-01],
          [-1.4553e-01,  2.7810e-01],
          [-1.4165e-01, -4.4159e-01],
          [-5.4208e-01,  1.0320e+00]],

         [[-8.0137e-01, -1.2661e+00],
          [-1.0142e+00, -4.0317e-01],
          [ 9.1506e-01,  5.2039e-01],
          [ 9.2211e-02, -1.8883e-01]],

         [[ 2.5000e-01, -8.7749e-01],
          [-7.5105e-01, -4.7226e-04],
          [ 3.9992e-01,  9.7360e-01],
          [ 6.6191e-01, -6.1057e-01]],

         [[-1.2200e+00,  4.3458e-01],
          [-1.4780e-02,  2.6286e-01],
          [ 4.1772e-01,  2.5125e-01],
          [-8.6761e-01,  6.2330e-02]]],


        [[[ 3.4582e-01, -8.6573e-01],
          [-9.4426e-02, -1.3522e-01],
          [-3.0205e-01, -6.8622e-02],
          [ 5.6740e-02,  7.6430e-02]],

         [[ 5.4049e-01,  4.0496e-01],
          [-1.0218e-01,  3.0620e-02],
          [-3.2552e-01,  1.7594e-01],
          [ 1.0315e+00,  1.7801e-01]],

         [[ 3.0589e-01,  3.6384e-01],
          [ 1.1659e+00, -8.6477e-01],
          [-1.0766e+00, -1.0674e+00],
          [-1.0885e+00, -1.8474e-01]],

         [[ 2.6526e-01,  6.1404e-02],
          [-1.2641e+00,  5.9070e-01],
          [-1.2353e+00,  1.3355e+00],
          [-4.0602e-01,  7.4847e-01]]]], device=device)

attn_mask = torch.tensor([[[[-inf, -inf, 0., -inf],
          [0., 0., 0., -inf],
          [-inf, -inf, -inf, -inf],
          [0., 0., -inf, 0.]]]], device=device)  

dropout_p = 0.0  
is_causal = False  
with sdpa_kernel([SDPBackend.MATH]):
    attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)
print(attn_output)