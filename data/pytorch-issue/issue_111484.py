import torch.nn as nn

import torch

batch_size = 70
seq_len = 2048
vocab_size = 50000

shift_labels = torch.zeros(batch_size, seq_len-1, dtype=torch.long).to("cuda")
logits = torch.ones(batch_size, seq_len-1, vocab_size, dtype=torch.float16).to("cuda")

loss_fct = torch.nn.CrossEntropyLoss(reduction="none")

# Evaluate loss first time
nll = loss_fct(logits.permute(0, 2, 1), shift_labels).float()
print(nll)
# This gives:
# tensor([[10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#       [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#       [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        ...,
#       [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],
#       [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],
#       [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000]])

# Evaluate loss second time
nll = loss_fct(logits.permute(0, 2, 1), shift_labels).float()
print(nll)
# This gives
# tensor([[10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        ...,
#        [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
#        [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],
#        [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]])

# tensor([[10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        ...,
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203],
#        [10.8203, 10.8203, 10.8203,  ..., 10.8203, 10.8203, 10.8203]])