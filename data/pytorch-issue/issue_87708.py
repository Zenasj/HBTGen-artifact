import torch

QConfig(
        activation=Quantizers.FakeQuantize.with_args(
            observer=Observers.MovingAverageMinMaxObserver.with_args(
                dtype=torch.qint8),
            qscheme=torch.per_tensor_symmetric,
            quant_min=-127,
            quant_max=127,
            dtype=torch.qint8

        ),
        weight=Quantizers.FakeQuantize.with_args(
            observer=Observers.MovingAveragePerChannelMinMaxObserver,
            quant_min=-127,
            quant_max=127,
            dtype=torch.qint8,
            qscheme=torch.per_channel_symmetric,
            reduce_range=False,
            ch_axis=0
        ))

qnnpack_act_qint8_scale_min_2_neg_12 = DTypeWithConstraints(
    dtype=torch.qint8,
    scale_min_lower_bound=2 ** -12,
)

qnnpack_weight_qint8_neg_127_to_127_scale_min_2_neg_12 = DTypeWithConstraints(
    dtype=torch.qint8,
    quant_min_lower_bound=-127,
    quant_max_upper_bound=127,
    scale_min_lower_bound=2 ** -12,
)

qnnpack_weighted_op_qint8_symmetric_dtype_config = DTypeConfig(
    input_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
    output_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
    weight_dtype=qnnpack_weight_qint8_neg_127_to_127_scale_min_2_neg_12,
    bias_dtype=torch.float,
)

qnnpack_default_op_qint8_symmetric_dtype_config = DTypeConfig(
    input_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
    output_dtype=qnnpack_act_qint8_scale_min_2_neg_12,
)

['ABC',
 'ABCMeta',
 'Any',
 'Callable',
 'DEFAULT_DYNAMIC_QUANT_MODULE_MAPPINGS',
 'DEFAULT_DYNAMIC_SPARSE_QUANT_MODULE_MAPPINGS',
 'DEFAULT_FLOAT_TO_QUANTIZED_OPERATOR_MAPPINGS',
 'DEFAULT_MODULE_TO_ACT_POST_PROCESS',
 'DEFAULT_OP_LIST_TO_FUSER_METHOD',
 'DEFAULT_PATTERN_TO_FUSER_METHOD',
 'DEFAULT_QAT_MODULE_MAPPINGS',
 'DEFAULT_REFERENCE_STATIC_QUANT_MODULE_MAPPINGS',
 'DEFAULT_STATIC_QUANT_MODULE_MAPPINGS',
 'DEFAULT_STATIC_SPARSE_QUANT_MODULE_MAPPINGS',
 'DeQuantStub',
 'Dict',
 'F',
 'FakeQuantize',
 'FakeQuantizeBase',
 'FixedQParamsFakeQuantize',
 'FixedQParamsObserver',
 'FusedMovingAvgObsFakeQuantize',
 'HistogramObserver',
 'List',
 'MatchAllNode',
 'MinMaxObserver',
 'Module',
 'MovingAverageMinMaxObserver',
 'MovingAveragePerChannelMinMaxObserver',
 'NoopObserver',
 'ObserverBase',
 'Optional',
 'OrderedDict',
 'Pattern',
 'PerChannelMinMaxObserver',
 'PlaceholderObserver',
 'QConfig',
 'QConfigAny',
 'QConfigDynamic',
 'QuantStub',
 'QuantType',
 'QuantWrapper',
 'RecordingObserver',
 'ReuseInputObserver',
 'Set',
 'Tuple',
 'Type',
 'UniformQuantizationObserverBase',
 'Union',
 '__builtins__',
 '__cached__',
 '__doc__',
 '__file__',
 '__loader__',
 '__name__',
 '__package__',
 '__path__',
 '__spec__',
 'abstractmethod',
 'activation_is_memoryless',
 'add_module_to_qconfig_obs_ctr',
 'add_observer_',
 'add_quant_dequant',
 'ao_nn',
 'assert_valid_qconfig',
 'backend_config',
 'calculate_qmin_qmax',
 'check_min_max_valid',
 'convert',
 'convert_dynamic_jit',
 'convert_jit',
 'copy',
 'default_activation_only_qconfig',
 'default_affine_fixed_qparams_fake_quant',
 'default_affine_fixed_qparams_observer',
 'default_debug_observer',
 'default_debug_qconfig',
 'default_dynamic_fake_quant',
 'default_dynamic_qat_qconfig',
 'default_dynamic_qconfig',
 'default_dynamic_quant_observer',
 'default_embedding_fake_quant',
 'default_embedding_fake_quant_4bit',
 'default_embedding_qat_qconfig',
 'default_embedding_qat_qconfig_4bit',
 'default_eval_fn',
 'default_fake_quant',
 'default_fixed_qparams_range_0to1_fake_quant',
 'default_fixed_qparams_range_0to1_observer',
 'default_fixed_qparams_range_neg1to1_fake_quant',
 'default_fixed_qparams_range_neg1to1_observer',
 'default_float_qparams_observer',
 'default_float_qparams_observer_4bit',
 'default_fused_act_fake_quant',
 'default_fused_per_channel_wt_fake_quant',
 'default_fused_wt_fake_quant',
 'default_histogram_fake_quant',
 'default_histogram_observer',
 'default_observer',
 'default_per_channel_qconfig',
 'default_per_channel_symmetric_qnnpack_qat_qconfig',
 'default_per_channel_symmetric_qnnpack_qconfig',
 'default_per_channel_weight_fake_quant',
 'default_per_channel_weight_observer',
 'default_placeholder_observer',
 'default_qat_qconfig',
 'default_qat_qconfig_v2',
 'default_qconfig',
 'default_reuse_input_observer',
 'default_reuse_input_qconfig',
 'default_symmetric_fixed_qparams_fake_quant',
 'default_symmetric_fixed_qparams_observer',
 'default_symmetric_qnnpack_qat_qconfig',
 'default_symmetric_qnnpack_qconfig',
 'default_weight_fake_quant',
 'default_weight_observer',
 'default_weight_only_qconfig',
 'disable_fake_quant',
 'disable_observer',
 'enable_fake_quant',
 'enable_observer',
 'enum',
 'fake_quantize',
 'float16_dynamic_qconfig',
 'float16_static_qconfig',
 'float_qparams_weight_only_qconfig',
 'float_qparams_weight_only_qconfig_4bit',
 'fuse_conv_bn',
 'fuse_conv_bn_jit',
 'fuse_conv_bn_relu',
 'fuse_convtranspose_bn',
 'fuse_linear_bn',
 'fuse_modules',
 'fuse_modules_qat',
 'fused_per_channel_wt_fake_quant_range_neg_127_to_127',
 'fused_wt_fake_quant_range_neg_127_to_127',
 'fuser_method_mappings',
 'fx',
 'get_combined_dict',
 'get_default_compare_output_module_list',
 'get_default_dynamic_quant_module_mappings',
 'get_default_dynamic_sparse_quant_module_mappings',
 'get_default_float_to_quantized_operator_mappings',
 'get_default_qat_module_mappings',
 'get_default_qat_qconfig',
 'get_default_qat_qconfig_dict',
 'get_default_qconfig',
 'get_default_qconfig_dict',
 'get_default_qconfig_propagation_list',
 'get_default_static_quant_module_mappings',
 'get_default_static_quant_reference_module_mappings',
 'get_default_static_sparse_quant_module_mappings',
 'get_dynamic_quant_module_class',
 'get_embedding_qat_module_mappings',
 'get_embedding_static_quant_module_mappings',
 'get_fuser_method',
 'get_fuser_method_new',
 'get_observer_dict',
 'get_observer_state_dict',
 'get_qparam_dict',
 'get_quantized_operator',
 'get_static_quant_module_class',
 'get_unique_devices_',
 'get_valid_patterns',
 'has_no_children_ignoring_parametrizations',
 'is_activation_post_process',
 'is_reuse_input_qconfig',
 'itertools',
 'load_observer_state_dict',
 'namedtuple',
 'nn',
 'nni',
 'nniq',
 'nniqat',
 'nniqd',
 'nnq',
 'nnqat',
 'nnqatd',
 'nnqd',
 'nnqr',
 'no_observer_set',
 'observer',
 'partial',
 'per_channel_dynamic_qconfig',
 'per_channel_weight_observer_range_neg_127_to_127',
 'prepare',
 'prepare_dynamic_jit',
 'prepare_jit',
 'prepare_qat',
 'propagate_qconfig_',
 'qconfig',
 'qconfig_dict_utils',
 'qconfig_equals',
 'quant_type',
 'quant_type_to_str',
 'quantization_mappings',
 'quantization_types',
 'quantize',
 'quantize_dynamic',
 'quantize_dynamic_jit',
 'quantize_fx',
 'quantize_jit',
 'quantize_qat',
 're',
 'register_activation_post_process_hook',
 'reverse2',
 'reverse3',
 'reverse_sequential_wrapper2',
 'script_qconfig',
 'script_qconfig_dict',
 'sequential_wrapper2',
 'stubs',
 'swap_module',
 'torch',
 'type_before_parametrizations',
 'utils',
 'warnings',
 'weight_observer_range_neg_127_to_127',
 'wrap_cpp_module']

from torch.ao.quantization.qconfig_mapping import _get_symmetric_qnnpack_qconfig_mapping
from torch.ao.quantization.backend_config import get_qnnpack_backend_config
from torch.ao.quantization.quantize_fx import prepare_qat_fx

# May not be needed in the future
torch.backends.quantized.engine = "qnnpack"

model = ...
example_inputs = ...
qconfig_mapping = _get_symmetric_qnnpack_qconfig_mapping()
backend_config = get_qnnpack_backend_config()
model_prepared = prepare_qat_fx(
    model,
    qconfig_mapping,
    example_inputs,
    backend_config=backend_config)

model_fp = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)

torch.backends.quantized.engine="qnnpack"
inputs = torch.rand((1, 3, 224, 224))

qconfig_mapping = _get_symmetric_qnnpack_qconfig_mapping()
backend_config = get_qnnpack_backend_config()

model_prepared = quantize_fx.prepare_qat_fx(
    model_fp,
    qconfig_mapping, 
    inputs,
    backend_config=backend_config)

model_prepared.eval()
model_quantized = quantize_fx.convert_fx(model_prepared)