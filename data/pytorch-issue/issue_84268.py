import torch
import torch.nn as nn

class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: ResidualAttentionBlock):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)

class LayerNorm(nn.Module):
    def __init__(self, *args, **kwargs):
        super(LayerNorm, self).__init__()
        self.inner_layernorm = nn.LayerNorm(*args, **kwargs)

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = self.inner_layernorm(x.type(torch.float32))
        return ret.type(orig_type)