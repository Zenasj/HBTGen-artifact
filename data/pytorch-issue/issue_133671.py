import torch.nn as nn

import torch
from torch.nn.attention import SDPBackend, sdpa_kernel

def test_scaled_dot_product_attention(
    sel_device,
):
    # Input tensors that are generated randomly
    torch.manual_seed(777)
    in_query = (
        torch.empty(size=[2, 2, 49, 32], dtype=torch.float32, device="cpu")
        .uniform_(-1, 1)
        .to(device=sel_device)
        .requires_grad_(True)
    )
    in_key = (
        torch.empty(size=[2, 2, 49, 32], dtype=torch.float32, device="cpu")
        .uniform_(-1, 1)
        .to(device=sel_device)
        .requires_grad_(True)
    )
    in_value = (
        torch.empty(size=[2, 2, 49, 32], dtype=torch.float32, device="cpu")
        .uniform_(-1, 1)
        .to(device=sel_device)
        .requires_grad_(True)
    )
    def fwd_subgraph():
        # pytorch op calls encoded from aten functions
        out = torch.nn.functional.scaled_dot_product_attention(
            in_query, in_key, in_value, None, 0.0, False
        )
        return out
    res = fwd_subgraph()
    # Gradient tensors (for outputs) that are generated randomly
    if res.requires_grad:
        res_grad = (
            torch.empty_like(res, device="cpu")
            .uniform_(-1, 1)
            .to(device=sel_device)
        )
        res.backward(res_grad, retain_graph=True)
    return {
        "res": res,
        "in_value.grad": in_value.grad,
        "in_key.grad": in_key.grad,
        "in_query.grad": in_query.grad,
    }
with sdpa_kernel([SDPBackend.FLASH_ATTENTION]):
    cpu_result_1 = test_scaled_dot_product_attention(torch.device("cpu"),)
print(cpu_result_1["in_value.grad"])
with sdpa_kernel([SDPBackend.MATH]):
    cpu_result_2 = test_scaled_dot_product_attention(torch.device("cpu"),)
print(cpu_result_2["in_value.grad"])

tensor([[[[ 5.9328e-02,  7.9576e-02, -3.7140e-04,  ..., -1.1795e-01,
            7.7002e-02,  7.2474e-02],
          [-1.9204e-02,  4.6380e-02,  1.4930e-03,  ..., -1.4451e-01,
            1.0098e-01,  1.0014e-01],
          [ 8.8671e-03,  5.9661e-02, -3.6109e-02,  ..., -1.1923e-01,
            3.7244e-02,  5.9829e-02],
          ...,
          [-7.3055e-03,  3.1238e-02, -1.2273e-03,  ..., -1.0132e-01,
            4.0625e-02,  1.0333e-01],
          [-4.5186e-02,  1.0394e-01, -1.1886e-01,  ..., -7.7129e-02,
            3.5467e-02,  3.7952e-02],
          [ 3.4043e-03,  6.3674e-02,  5.2469e-03,  ..., -1.3449e-01,
            6.1767e-02,  9.1462e-02]],

         [[ 8.0023e-02, -8.4533e-02,  1.1002e-01,  ..., -1.2878e-01,
           -5.5690e-03, -7.2777e-02],
          [ 8.9278e-02, -5.9432e-02,  5.2141e-02,  ..., -9.7144e-02,
           -3.3755e-02, -4.6284e-02],
          [ 9.7849e-02,  4.1231e-03,  6.2617e-02,  ..., -1.2769e-01,
           -6.0783e-02, -8.9587e-02],
          ...,
          [ 9.8016e-02, -6.1659e-02,  7.1336e-02,  ..., -1.3613e-01,
            2.5708e-02, -7.0694e-02],
          [ 9.1727e-02, -2.0901e-02,  6.4952e-02,  ..., -7.8813e-02,
           -5.8659e-02, -6.2331e-02],
          [ 7.7773e-02, -1.1136e-01,  1.1785e-01,  ..., -6.5297e-02,
           -2.6213e-02, -4.4343e-02]]],


        [[[ 8.6087e-02, -1.0936e-01, -9.6405e-02,  ..., -3.0870e-02,
            6.9013e-03,  1.1481e-01],
          [ 6.6898e-02, -4.3557e-02, -3.8033e-02,  ..., -3.5249e-02,
           -6.1041e-02,  1.2622e-01],
          [ 3.3132e-02, -1.0397e-01, -4.6597e-02,  ..., -2.2557e-02,
           -7.7216e-02,  1.3977e-01],
          ...,
          [ 6.9947e-02, -9.7220e-02, -4.9563e-02,  ..., -4.4335e-03,
           -3.6696e-02,  1.3165e-01],
          [ 6.8503e-02, -1.0009e-01, -1.0861e-01,  ...,  1.4641e-03,
            1.4435e-04,  1.6588e-01],
          [ 5.3724e-02, -1.1875e-01, -1.4079e-01,  ..., -2.0743e-02,
           -1.2378e-02,  1.4002e-01]],

         [[-8.9311e-02,  1.3538e-01,  7.5159e-02,  ..., -4.6596e-02,
           -1.5554e-01,  1.8591e-01],
          [-4.0929e-02,  1.3667e-01,  5.3036e-02,  ..., -6.6295e-02,
           -5.8507e-02,  1.2799e-01],
          [-1.3276e-02,  1.1356e-01,  3.5853e-02,  ..., -4.7057e-02,
           -1.4666e-01,  1.4465e-01],
          ...,
          [-4.0269e-02,  7.6953e-02,  6.2881e-02,  ..., -3.6842e-02,
           -3.5524e-02,  1.2240e-01],
          [-7.0776e-02,  1.5214e-01,  7.6208e-02,  ..., -7.9796e-02,
           -8.1043e-02,  1.6276e-01],
          [-1.9021e-02,  9.1104e-02,  3.1475e-02,  ..., -9.7966e-03,
           -2.5992e-02,  8.2071e-02]]]])
tensor([[[[ 0.0627,  0.0228, -0.0004,  ..., -0.0542,  0.0334,  0.0731],
          [ 0.0631, -0.0204,  0.0080,  ..., -0.0729,  0.0385, -0.0232],
          [ 0.0339, -0.0264,  0.0179,  ..., -0.1135,  0.0255, -0.0148],
          ...,
          [ 0.0638, -0.0313,  0.0760,  ..., -0.0805,  0.0734,  0.0015],
          [ 0.0039,  0.0563, -0.0023,  ..., -0.0990,  0.0502,  0.0599],
          [ 0.0269,  0.0147, -0.0308,  ..., -0.0806,  0.0164,  0.0289]],

         [[ 0.0710,  0.0104,  0.0150,  ..., -0.1195, -0.0466,  0.0174],
          [ 0.0972, -0.0113,  0.0481,  ..., -0.1013,  0.0099, -0.0395],
          [ 0.0660, -0.0048,  0.0343,  ..., -0.0805, -0.0084, -0.0605],
          ...,
          [ 0.0572, -0.0310,  0.0449,  ..., -0.1343,  0.0028, -0.0148],
          [ 0.0638,  0.0146,  0.0833,  ..., -0.0968, -0.0240, -0.0132],
          [ 0.1070,  0.0191,  0.0503,  ..., -0.1426, -0.0879,  0.0162]]],


        [[[ 0.0918,  0.1045, -0.1197,  ..., -0.0843, -0.0068,  0.0866],
          [ 0.0480,  0.0351, -0.0830,  ..., -0.1545, -0.0321,  0.0564],
          [ 0.0496,  0.0388, -0.0783,  ..., -0.1374, -0.0374,  0.0566],
          ...,
          [ 0.0365,  0.0531, -0.0878,  ..., -0.0787, -0.0337,  0.0951],
          [ 0.0630,  0.0616, -0.0866,  ..., -0.1836,  0.0205,  0.0496],
          [ 0.0461,  0.0467, -0.0893,  ..., -0.1381,  0.0169,  0.0547]],

         [[-0.0353,  0.0257,  0.1444,  ...,  0.0660, -0.1390,  0.2305],
          [-0.0712, -0.0305,  0.0765,  ...,  0.0736, -0.1068,  0.1931],
          [-0.0777,  0.0278,  0.0959,  ...,  0.0664, -0.1508,  0.2047],
          ...,
          [-0.0101, -0.0560,  0.0645,  ...,  0.0694, -0.1144,  0.1581],
          [-0.0576, -0.0219,  0.1573,  ...,  0.0403, -0.1028,  0.2323],
          [-0.0575, -0.0032,  0.0288,  ...,  0.0329, -0.0532,  0.1165]]]])

res_grad = (
    torch.empty_like(res, device="cpu")
    .uniform_(-1, 1)
    .to(device=sel_device)
)
res.backward(res_grad, retain_graph=True)