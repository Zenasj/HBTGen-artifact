import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import os
import torch.nn.functional as F
gpu_device = torch.device("cuda")
cpu_device = torch.device("cpu")
class PreprocessAndCalculateModel(nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        device = x.device
        output =  torch.linalg.pinv(x)
        return output

real_inputs = torch.Tensor([[[[-1.6146, -0.9381, -1.5425, -0.8857, -1.0262, -1.1434, -0.7087,
           -1.4301],
          [-1.3076, -1.1190, -0.8301, -1.1322, -0.8404, -1.2032, -1.0934,
           -1.0547],
          [-1.0442, -1.5093, -1.3251, -1.3437, -1.3479, -1.2869, -1.3089,
           -0.9086],
          [-1.3969, -1.2437, -0.9029, -0.9959, -1.3150, -0.5801, -0.8783,
           -1.0645],
          [-0.9151, -1.2837, -0.9983, -1.0371, -1.1428, -0.8917, -0.7651,
           -1.5479],
          [-0.9188, -1.3187, -0.9929, -0.6102, -1.6534, -1.1305, -0.9660,
           -0.7143],
          [-1.0792, -1.2437, -0.9711, -1.1356, -0.8029, -1.5024, -1.1226,
           -1.2592],
          [-1.0208, -0.9037, -1.1391, -1.0418, -1.2953, -1.3398, -0.9300,
           -1.0324]],

         [[-0.9464, -0.9135, -1.0808, -1.3064, -1.6155, -1.1230, -1.1643,
           -0.7409],
          [-1.0700, -0.8563, -1.0147, -1.2051, -0.9755, -1.2394, -0.9041,
           -1.3153],
          [-1.1186, -0.7538, -1.2094, -0.9862, -1.3948, -1.1256, -1.0191,
           -1.4821],
          [-1.2525, -0.9754, -1.1601, -1.4554, -0.9923, -1.5012, -1.4452,
           -1.1151],
          [-1.2178, -0.9488, -1.0959, -1.5021, -1.3065, -1.4730, -1.0634,
           -0.7523],
          [-1.1835, -0.8102, -1.2250, -1.4858, -0.9443, -0.7896, -0.8500,
           -1.3230],
          [-1.6832, -0.9332, -1.4956, -0.7940, -1.0474, -0.8189, -1.3353,
           -0.7411],
          [-0.9188, -1.0427, -1.0066, -0.8061, -1.1196, -0.9813, -1.2219,
           -1.0846]],

         [[-0.8846, -1.5725, -0.8055, -1.1496, -0.8145, -1.0329, -1.6320,
           -1.2587],
          [-0.9506, -1.3917, -1.6020, -0.9728, -1.6531, -0.8911, -1.3470,
           -0.9589],
          [-1.1355, -1.1765, -0.8304, -1.0048, -0.7086, -0.9177, -0.9970,
           -0.9949],
          [-0.7618, -1.0947, -1.2689, -0.9231, -1.0193, -1.5265, -1.0532,
           -1.1171],
          [-1.1919, -1.0914, -1.2132, -0.8607, -0.8909, -1.0195, -1.6638,
           -1.1519],
          [-1.2216, -1.2456, -1.0914, -1.4678, -0.8684, -1.5002, -1.6502,
           -1.4101],
          [-0.7458, -1.1445, -0.9232, -1.4840, -1.6039, -1.0893, -0.8879,
           -1.4291],
          [-1.4242, -1.4171, -1.1570, -1.6066, -0.9169, -1.0125, -1.1687,
           -1.1847]]],


        [[[-1.2687, -1.1064, -0.9705, -1.0390, -0.9778, -1.1705, -0.8000,
           -1.5272],
          [-0.6351, -0.8533, -1.1124, -0.8990, -1.0173, -1.0989, -1.2648,
           -0.6185],
          [-1.3267, -1.6984, -1.0927, -1.1768, -0.8752, -1.0548, -1.3437,
           -1.3235],
          [-1.2667, -0.9677, -1.0421, -0.9875, -0.9691, -1.2230, -0.9481,
           -1.2245],
          [-1.1482, -1.1788, -1.0590, -1.7281, -1.2796, -1.0019, -1.1788,
           -0.8423],
          [-1.5063, -1.1839, -1.4684, -1.2288, -1.0284, -1.2785, -0.8742,
           -1.4627],
          [-0.8454, -1.5394, -1.0514, -1.0141, -1.2248, -0.9020, -1.1741,
           -0.9963],
          [-1.5857, -1.0084, -1.1886, -1.0803, -1.2557, -1.0109, -1.1922,
           -0.6307]],

         [[-0.8593, -1.2153, -1.5561, -1.4144, -1.0144, -0.7991, -1.2830,
           -0.9455],
          [-1.4842, -1.1828, -1.1948, -0.9603, -1.3431, -1.2849, -1.0593,
           -1.4786],
          [-0.9822, -1.0154, -1.2840, -0.9746, -1.0761, -1.0396, -0.9412,
           -0.7584],
          [-1.1592, -1.2620, -1.1893, -1.2004, -0.9949, -0.8363, -1.3427,
           -1.1323],
          [-1.0830, -1.2063, -1.2392, -1.0755, -0.7922, -0.9064, -0.9731,
           -1.6946],
          [-1.4568, -0.9528, -1.0114, -0.7879, -1.0040, -1.3847, -0.9697,
           -1.3419],
          [-1.3543, -0.9581, -1.2405, -1.0364, -1.1954, -1.6610, -0.9302,
           -1.2228],
          [-1.3160, -1.0681, -1.4304, -1.5617, -1.3287, -1.0630, -1.1371,
           -1.5429]],

         [[-1.2197, -0.9872, -0.8912, -0.9085, -1.3422, -1.4270, -1.2965,
           -0.9305],
          [-1.4129, -1.3183, -0.9985, -1.5595, -0.9745, -0.9414, -0.9915,
           -1.4556],
          [-1.0212, -0.7879, -0.9474, -1.1571, -1.4176, -1.2101, -1.0528,
           -1.3266],
          [-0.9050, -1.0878, -1.0705, -1.1196, -1.3831, -1.3007, -1.0458,
           -0.9577],
          [-1.0665, -0.9338, -1.0118, -0.7314, -1.3131, -1.4747, -1.1569,
           -0.9529],
          [-0.6064, -1.1768, -0.9014, -1.3762, -1.2873, -0.7526, -1.5915,
           -0.6792],
          [-1.1632, -0.9116, -1.0180, -1.2639, -0.9074, -0.9057, -1.2160,
           -1.0896],
          [-0.6406, -1.2325, -0.7850, -0.7969, -0.7979, -1.2355, -0.9790,
           -1.3703]]]])
model = PreprocessAndCalculateModel()
x = real_inputs
output_gpu = model.to(gpu_device)(x.cuda())
output_cpu = model.to(cpu_device)(x.cpu())
print(torch.allclose(output_gpu.cpu(), output_cpu, atol=1e-1))